{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b9243b7-c456-4597-8ce0-369e0a5405dc",
   "metadata": {},
   "source": [
    "State at time $t$: $s_t=(\\begin{bmatrix}y_1\\\\...\\\\y_t-1\\end{bmatrix},\\begin{bmatrix}\\hat{w_1}^T\\\\...\\\\\\hat{w_{t-1}}^T\\end{bmatrix})$\n",
    "\n",
    "Action given $s_t$: row index of $W$\n",
    "\n",
    "Observation is $y_t=\\hat{W_t}\\vec{x}$\n",
    "\n",
    "Get new state $s_{t+1}$\n",
    "\n",
    "Policy function: map given state to probabilities that action will be picked from that state, $\\pi(a|s)$ at time $t$. For each $s\\in S$, policy $\\pi$ is a probability distribution over $a \\in A$.\n",
    "\n",
    "- pick logN rows first <-- limits possible policies\n",
    "\n",
    "Store all episodes to get expected return, use policy gradient to improve it. Don't use rewards with policy gradient\n",
    "\n",
    "Look into different RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93efcfe-0902-4f18-b585-d74615657dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
